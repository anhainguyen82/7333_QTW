{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center\"> DS7333 - Case Study 15 | QTW Final Project</div>\n",
    "### <div style=\"text-align:center\">Andy Ho, An Nguyen, Jodi Pafford</div>\n",
    "<div style=\"text-align:center\">August 12, 2019</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This report will provide our methods, model, analysis, and conclusions for the unlabeled data set provided.  Along with that, the data cleaning process will be explained and details on our findings provided.  <br><br>\n",
    "To recap, we were assigned a dataset with 160,000 records and 51 features.  According to our business partner, the data is related to the insurance industry.  For the most part, the features were unlabeled – meaning the data has not been tagged identifying its characteristics, properties, or classifications.  However, the target variable (i.e. dependent variable) is labeled (0 or 1).  As a result, we will be looking at various supervised machine learning methods to arrive at our conclusions. <br><br>\n",
    "For general background, supervised machine learning (SML) learns from the dataset to for classification and/or regression purposes.  In this report, we will use the following methods for classification:\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Support Vector Machine\n",
    "- K- Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective given by our business partner is to arrive at a model that best classifies the ‘y’ values (0 or 1).  Hence, this is a classification problem and our conclusions will report the model with the best accuracy in classifying records/observations.  <br><br>\n",
    "What does classification mean?  At its core, classification learns from the data given (i.e. training dataset) and then uses the findings to create a model to accurately classify new records.  Along with accuracy, we will show the ratio of actual values to predicted values.  As advised by our business partner, it is important to the business to accurately classify but even more so to lower the amount of ‘False Positives’ – mis-classifying a false positive could cost the business a thousand dollars while mis-classifying a false negative cost hundred dollars.  Therefore, our objective is to arrive at the highest accuracy score with the lowest false positve ratio.  Below is a graphic to help explain the possible outcomes:\n",
    "![alt text](classification_outcomes.png \"Classification Outcomes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Our recommendation is using K-NN (k=9) with feature selection to best classify for this data set.  Based on receiving new data with these same categories, the business will be able to predict/classify with approximately a 95.12% accuracy whether the transaction will result in money lost (0) or gained (1). Out of the 5% error rate in prediction, 4.07% were false positive results and 6.56% were false negative results.<br><br>\n",
    "The remainder of the report will provide further details on our process of data cleaning, feature selection/engineering, methods, modeling, and analysis.  \n",
    "## Our Process\n",
    "The objective is to suggest/recommend Machine Learning models that would most accurately classify (with the lowest 'False Positives') the outcome ‘y.’   Our process was as follows:\n",
    "\n",
    "1.\tClean Data\n",
    "2.\tFeature Select\n",
    "3.\tModel\n",
    "4.\tAnalyze\n",
    "5.\tEvaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "The raw data set was unlabeled.  It contained a total of 160,000 records and 51 features.  Using a Python library called ‘Pandas_Profiling’, we uncovered the following:\n",
    "1.\tDatatypes <br>\n",
    "    a.\tNumeric: 45<br>\n",
    "    b.\tCategorical: 5<br>\n",
    "    c.\tBoolean: 1<br>\n",
    "2.\tHigh Correlation<br>\n",
    "    a.\tx41 – Dropped x41 from dataset<br>\n",
    "    b.\tx6 – Dropped x6 from dataset<br>\n",
    "3.\tHigh Cardinality<br>\n",
    "    a.\tx37 – The values had incorrect datatype – causing an incorrect assesessment of high cardinality.  To clean it, we removed the commas and dollar sign, and replaced the parenthesis with a minus symbol.  Converted datatype to numeric.<br>\n",
    "4.\tData augmentation<br>\n",
    "    a.\tMisspellings were present in x24.  Corrected ‘euorpe’ to ‘europe’<br>\n",
    "    b.\tStandardize values<br>\n",
    "        1.\tx29 – Replaced all month abbreviations to full spelling (i.e. Aug to August)<br>\n",
    "        2.\tx30 – Corrected misspelling of ‘thurday’ to ‘thursday’<br>\n",
    "5.\tMissing values, NaN:<br>\n",
    "    a.\tThere are 20-40 missing values in each column.<br>\n",
    "    b.\tMost of the missing values appears to be from Asia.  It was decided that these data can be replaced with the mean of the available data from Asia.<br>\n",
    "    c.\tThe observations with missing values in the column with different continent names were dropped.<br>\n",
    "6.\tData Distribution was normal.  No data transformation was required.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Select\n",
    "This portion took a bulk of our time.  The process of feature selection was a continuous cycle between ‘feature select’ and ‘modeling.’  The first pass was to use the complete dataset with each subsequent iteration removing features and/or records.<br><br>\n",
    "\tWe attempted to reduce the number of features required through PCA.  We found that 8 principal components can explain for 99.9% of the variance.<br><br>\n",
    "Because this was a classification assignment, we selected a few classification models to find the best model for the data set (refer to ‘Model’ section for more details).  In the end, we decided that all the categorical features made little to no impact on the accuracy scores – therefore, dropped.<br><br>\n",
    "**Random Forest Feature Importance (RFFI)**<br>\n",
    "Knowing feature importance allowed us to better understand the data set and provided insights to our feature selection process.  After removing the categorical features, we ran RFFI for the remaining 45 features (using all 160,000 records) to arrive at the following feature of importance (>0.04):<br>\n",
    "- x23\n",
    "- x20\n",
    "- x48\n",
    "- x49\n",
    "- x38\n",
    "- x12\n",
    "- x42\n",
    "- x27\n",
    "- x40\n",
    "- x37\n",
    "- x28\n",
    "- x7\n",
    "- x2\n",
    "- x46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling of Methods\n",
    "We started to evaluate the SML methods stated above from the most common/basic to more complex.  Each method selected utilized two versions of the data set to arrive at the method that yielded the best model (i.e. most accurate). The following will provide further detail into our process in sequential order of evaluation:<br><br>\n",
    "**Logistic Regression (LR)**<br>\n",
    "This method was the most obvious to run first since the dependent variable (‘y) was binary (0 or 1). The objective is to describe data and to explain the relationship between the dependent variable to the rest of the data set. Unfortunately, LR yielded the lowest accuracy rates in predicting the classification.\n",
    "- Full Data Set: 70.27%\n",
    "- Feature Selected: 70.33%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree (DT)**<br>\n",
    "The goal of DT is to split the observations in a way that the resulting groups are as different from each other as much as possible.  The structure of a tree is the root node, internal node, and leaf nodes – all of which are connected by branches.  DT is the building block for Random Forest.  The DT yielded better accuracy results compared to LR.\n",
    "- Full Data: 84.09% accuracy using entropy\n",
    "- Feature Selected: 86.5% accuracy using entropy criterion.\n",
    "- XGBoosted on Feature Selected: 83.12%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest(RF)**<br>\n",
    "RF uses a large number of uncorrelated decision trees to operate as a committee – much like an ensemble.  Each DT outputs a class prediction and the class with the most votes is the RF’s optimal model prediction.  The RF yielded better accuracy results compared to both LR and DT.\n",
    "- Full Data: 88.7% accuracy\n",
    "- Feature Selected: 91.5% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine (SVM)**<br>\n",
    "**AN ADD INFO HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)**<br>\n",
    "**AN ADD INFO HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbor (KNN)**<br>\n",
    "KNN is a non-parametric, lazy learning machine learning algorithm with the purpose to predict the classification of data. Non-parametric means that no assumptions are made about the data before classifying; lazy learning means that there is no training phase before classifying. KNN often has the ‘dimensionality curse’, where a dataset with many variables is not as accurate; however, because our dataset is so large, we reduce the dimensionality curse. KNN is our ‘winning’ algorithm (see Analysis section for more details on the process). Other methods tried with KNN were adjusting the test size, one-hot-encoding all categorical variables, and using \n",
    "- Full Data: 80.24% accuracy\n",
    "- Feature Selected: 95.17% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "With KNN, we started with the full dataset and k=5. After yielding somewhat high results on the first pass at the full dataset, we ran a KNN loop (with just the feature selected data) from k=1=k=25. Results can be seen below:<br><br>\n",
    "![alt text](KNN_Accuracy.png \"KNN Accuracy Chart\")\n",
    "The best accuracy was found at k=9, although not much variation occurred between k=5 and k=9. After k=9 the accuracy slowly dropped. Upon running KNN with the featured dataset, with a test size of 20% (32,000 samples), and a random_state set at “1234”, we found the accuracy to be 95.17%.<br>\n",
    "![alt text](KNN_Matrix.png \"KNN Confusion Matrix\")\n",
    "\n",
    "## Results\n",
    "KNN had an accuracy of 95.17%. Within those results, a positive was correctly predicted 96.26% of the time, and a negative was correctly predicted 93.55% of the time. Incorrect classifications of positive results occurred 3.74% of the time, while incorrect classifications of negative results occurred 6.45% of the time.<br>\n",
    "\n",
    "## Conclusions\n",
    "You can expect to correctly predict a positive (y=1) outcome 96.17% of the time and correctly predict a negative (y=0) outcome 93.55% of the time. <br>\n",
    "Further investigations can be done by analyzing the data with the categorical variables (continents, month, day). These datapoints did not prove significant in our findings, however, they may be important to the business partner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling as pp\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.externals.six import StringIO \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from IPython.display import Image \n",
    "from pydot import graph_from_dot_data\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import random\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV\n",
    "df = pd.read_csv(\"final_project.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename obvious columns\n",
    "df.rename(columns={'x24': 'continent', 'x29': 'month', 'x30': 'day'}, inplace = True)\n",
    "#list(df.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize temp for x37 column\n",
    "temp_x37 = []\n",
    "\n",
    "# Remove $ ) , characters and replace '(' with '-' \n",
    "for i in range (0,len(df)) :\n",
    "    try :\n",
    "        n = df['x37'][i]\n",
    "        nstr = re.sub(r'[$|,|)]',r'', n)\n",
    "        nstr = re.sub(r'[(]',r'-',nstr)\n",
    "        #nstr= float(nstr)\n",
    "        temp_x37.append(nstr)\n",
    "    except :\n",
    "        nstr = ''\n",
    "        temp_x37.append(nstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify len of both x37 matches\n",
    "print(len(df['x37']))\n",
    "print(len(temp_x37))\n",
    "\n",
    "# Replace 'x37' with new values and convert to numeric\n",
    "df['x37'] = temp_x37\n",
    "df[\"x37\"] = pd.to_numeric(df[\"x37\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct misspellings and standardize values in labeled columns\n",
    "df['continent'].replace('euorpe', 'europe',inplace=True)\n",
    "df['month'].replace('Dev', 'December',inplace=True)\n",
    "df['month'].replace('Aug', 'August',inplace=True)\n",
    "df['month'].replace('Jun', 'June',inplace=True)\n",
    "df['month'].replace('Apr', 'April',inplace=True)\n",
    "df['month'].replace('Nov', 'November',inplace=True)\n",
    "df['month'].replace('sept.', 'September',inplace=True)\n",
    "df['month'].replace('Oct', 'October',inplace=True)\n",
    "df['month'].replace('Mar', 'March',inplace=True)\n",
    "df['day'].replace('thurday', 'thursday',inplace=True)\n",
    "\n",
    "# Fill NA with 'other' in labeled columns\n",
    "df['continent'] = df['continent'].fillna('other')\n",
    "df['month'] = df['month'].fillna('other')\n",
    "df['day'] = df['day'].fillna('other')\n",
    "\n",
    "# check unique values in labeled columns\n",
    "print (df['continent'].unique())\n",
    "print (df['month'].unique())\n",
    "print (df['day'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct misspellings and standardize values in labeled columns\n",
    "df['continent'].replace('euorpe', 'europe',inplace=True)\n",
    "df['month'].replace('Dev', '12',inplace=True)\n",
    "df['month'].replace('Aug', '8',inplace=True)\n",
    "df['month'].replace('Jun', '6',inplace=True)\n",
    "df['month'].replace('Apr', '4',inplace=True)\n",
    "df['month'].replace('Nov', '11',inplace=True)\n",
    "df['month'].replace('sept.', '9',inplace=True)\n",
    "df['month'].replace('Oct', '10',inplace=True)\n",
    "df['month'].replace('Mar', '3',inplace=True)\n",
    "df['month'].replace('January', '1',inplace=True)\n",
    "df['month'].replace('Feb', '2',inplace=True)\n",
    "df['month'].replace('May', '5',inplace=True)\n",
    "df['month'].replace('July', '7',inplace=True)\n",
    "df['month'].replace('December', '12',inplace=True)\n",
    "df['month'].replace('August', '8',inplace=True)\n",
    "df['month'].replace('June', '6',inplace=True)\n",
    "df['month'].replace('April', '4',inplace=True)\n",
    "df['month'].replace('November', '11',inplace=True)\n",
    "df['month'].replace('September', '9',inplace=True)\n",
    "df['month'].replace('October', '10',inplace=True)\n",
    "df['month'].replace('March', '3',inplace=True)\n",
    "df['day'].replace('thurday', 'thursday',inplace=True)\n",
    "\n",
    "\n",
    "# Fill NA with 'other' in labeled columns\n",
    "df['continent'] = df['continent'].fillna('other')\n",
    "df['month'] = df['month'].fillna('other')\n",
    "df['day'] = df['day'].fillna('other')\n",
    "df['month'].replace('other','0', inplace=True)\n",
    "\n",
    "# check unique values in labeled columns\n",
    "print (df['continent'].unique())\n",
    "print (df['month'].unique())\n",
    "print (df['day'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting data set by continent and print length of each\n",
    "cont = ['asia', 'america','europe', 'other']\n",
    "\n",
    "for n in cont :\n",
    "    temp = df['continent'] == n\n",
    "    df_temp = df[temp]\n",
    "    #df_[n] = df_temp\n",
    "    print (n, 'length is', len(df_temp))\n",
    "\n",
    "# Subsetting by continent    \n",
    "is_asia = df['continent']=='asia'\n",
    "df_asia = df[is_asia]\n",
    "\n",
    "is_europe = df['continent']=='europe'\n",
    "df_europe = df[is_europe]\n",
    "\n",
    "is_america = df['continent']=='america'\n",
    "df_america = df[is_america]\n",
    "\n",
    "is_other = df['continent']=='other'\n",
    "df_other = df[is_other]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simple and fast exploratory data analysis \n",
    "pp.ProfileReport(df_america)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploration into missing data. Ultimately found that keeping NA and calculating mean was not impactful of results.\n",
    "#find all missing data\n",
    "#for x in df:\n",
    "#    try:\n",
    "#        index=df.index[np.isnan(df[x])]\n",
    "#    except:\n",
    "#        index=df.index[pd.isnull(df[x])]\n",
    "#        \n",
    "#    print(x)\n",
    "#    print(index)\n",
    "#    print(\"Amount missing:\", len(index))\n",
    "#    print(\"Missing values from continent:\")\n",
    "#    print(df['continent'][index].value_counts())\n",
    "#    print(\"\")\n",
    "\n",
    "#missing data are mostly from asian continent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploration into missing data. Ultimately found that keeping NA and calculating mean was not impactful of results.\n",
    "\n",
    "#is_asia = df['continent']=='asia'\n",
    "#df_asia = df[is_asia]\n",
    "\n",
    "#get mean of variables from asia if variable contains numbers, exclude 'NaN'\n",
    "#get most occuring values of variables from asia if variable contains strings, exclude nulls\n",
    "#impute_values = {}\n",
    "#for x in df:\n",
    "#    if x != 'continent':\n",
    "#        try:\n",
    "#            impute_values[x] = df_asia[~np.isnan(df_asia[x])][x].mean()\n",
    "#        except:\n",
    "#            impute_values[x] = df_asia[~pd.isnull(df_asia[x])][x].max()\n",
    "#            \n",
    "#impute NaN and nulls with values obtained from above         \n",
    "#for x in df:\n",
    "#    if x != 'continent':\n",
    "#        try:\n",
    "#            index=df.index[np.isnan(df[x])]\n",
    "#        except:\n",
    "#            index=df.index[pd.isnull(df[x])]\n",
    "#        df[x][index] = impute_values[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with complete data set - Drop continent, x41, x6 per EDA suggestion.\n",
    "# Need to drop day, month, x32 since it's categorical\n",
    "lr_df = df.drop(['x41', 'x6', 'continent', 'day','x32'], axis=1)\n",
    "\n",
    "# Fill in NA with mean - LR needs values in each cell \n",
    "lr_df = lr_df.fillna(lr_df.mean())\n",
    "\n",
    "# Alternative - Drop all rows with NA\n",
    "lr_df_no = lr_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr_df['y']\n",
    "X = lr_df.drop('y', axis = 1)\n",
    "\n",
    "# Model Fitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Precision, recall, F-Measure and Support\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp Dataset for Random Forest.  Dropped the highly correlated features.  Replace NaN with mean of column\n",
    "#rf_df = df.drop(['x41', 'x6'], axis=1)\n",
    "#rf_df = rf_df.fillna(lr_df.mean())\n",
    "\n",
    "rf_df = lr_df\n",
    "\n",
    "# One-hot encode the data using pandas get_dummies\n",
    "#features = pd.get_dummies(rf_df)\n",
    "\n",
    "# Display the first 5 rows of the last 12 columns\n",
    "#features.iloc[:,5:].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature of Importance\n",
    "# ref: https://towardsdatascience.com/running-random-forests-inspect-the-feature-importances-with-this-code-2b00dd72b92e\n",
    "\n",
    "y = lr_df['y']\n",
    "X = lr_df.drop('y', axis = 1)\n",
    "\n",
    "# Splitting data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "## Import the random forest model.\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "## Initiating Random Forest Classifier. \n",
    "rf = RandomForestClassifier() \n",
    "\n",
    "## Fitting model on training data.\n",
    "rf.fit(X_train, y_train) \n",
    "\n",
    "## Accuracy Score\n",
    "rf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=123)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict(X_test)\n",
    "print (\"Accuracy is \", accuracy_score(y_test,y_pred)*100, 'with Gini Index.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_ent = DecisionTreeClassifier(criterion='entropy',random_state=123)\n",
    "dt_ent.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt_ent.predict(X_test)\n",
    "print (\"Accuracy is \", accuracy_score(y_test,y_pred)*100, 'with Information Gain.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree, Logistic Regressions, and Random Forest using Features of Importance\n",
    "Summary: Decision Tree yields the best accuracy at 86.85% using max_depth of 15 and entropy.  Logistic Regression showed tiny improvement from 70.27% to 70.33%.  Recommendation is to go with Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting by feature of importance from RF\n",
    "#rf_df_1 = lr_df [['x23', 'x12', 'x20', 'x48', 'x49', 'x27', 'x28','x37', 'x38', 'x42', 'x2', 'x7' ,'x46', 'x40', 'y']]\n",
    "\n",
    "# importance > .04\n",
    "rf_df_1 = lr_df [['x23', 'x20', 'x48', 'x49', 'x38', 'x12', 'x42', 'x27','x40', 'x37','x28','x7','x2', 'x46', 'y']]\n",
    "\n",
    "#rf_df_1 = lr_df [['x23', 'x20', 'x48', 'x49', 'x38', 'y']] - this set yielded a worse accuracy. importance > 0.05\n",
    "print(rf_df_1.head())\n",
    "print(rf_df_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree w/ Feature of Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_1 = rf_df_1['y']\n",
    "#X_1 = rf_df_1.drop('y', axis = 1)\n",
    "\n",
    "#rf_df_sample = rf_df.sample(frac=.95)\n",
    "\n",
    "y_1 = rf_df_1['y']\n",
    "X_1 = rf_df_1.drop('y', axis = 1)\n",
    "\n",
    "# Model Fitting\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_1, y_1, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=123)\n",
    "dt.fit(X_train1, y_train1)\n",
    "y_pred1 = dt.predict(X_test1)\n",
    "print (\"Accuracy is \", accuracy_score(y_test1,y_pred1)*100, 'with Gini Index.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_ent = DecisionTreeClassifier(criterion='entropy',random_state=123)\n",
    "dt_ent.fit(X_train1, y_train1)\n",
    "y_pred1 = dt_ent.predict(X_test1)\n",
    "print (\"Accuracy is \", accuracy_score(y_test1,y_pred1)*100, 'with Information Gain.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [5, 10, 11,12,13,14,15, 16, 17,18,19, 20, 22, 25, 50, 100]\n",
    "for n in n_est :\n",
    "    dt = DecisionTreeClassifier(random_state=123, max_depth = n)\n",
    "    dt.fit(X_train1, y_train1)\n",
    "    y_pred1 = dt.predict(X_test1)\n",
    "    print (\"Accuracy is \", accuracy_score(y_test1,y_pred1)*100, 'with Gini Index at', n, 'depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [5, 10, 11,12,13,14,15, 16, 17,18,19, 20, 22, 25, 50, 100]\n",
    "for n in n_est :\n",
    "    dt = DecisionTreeClassifier(criterion = 'entropy',random_state=123, max_depth = n)\n",
    "    dt.fit(X_train1, y_train1)\n",
    "    y_pred1 = dt.predict(X_test1)\n",
    "    print (\"Accuracy is \", accuracy_score(y_test1,y_pred1)*100, 'with Information Gain at', n, 'depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression w/ Feature of Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=123)\n",
    "logreg.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = logreg.predict(X_test1)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test1, y_test1)*100), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test1, y_pred1)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Precision, recall, F-Measure and Support\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "logit_roc_auc = roc_auc_score(y_test1, logreg.predict(X_test1))\n",
    "fpr, tpr, thresholds = roc_curve(y_test1, logreg.predict_proba(X_test1)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_graph_FI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest w/ Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the random forest model.\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "## Initiating Random Forest Classifier. \n",
    "rf = RandomForestClassifier() \n",
    "\n",
    "## Fitting model on training data.\n",
    "rf.fit(X_train1, y_train1) \n",
    "\n",
    "## Accuracy Score\n",
    "rf.score(X_test1, y_test1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCA = lr_df\n",
    "\n",
    "#variables to be drop\n",
    "#categoricals ('continent', 'day', 'month')\n",
    "#highly correlated ('x6', 'x41')\n",
    "#derived ('Total')\n",
    "#target ('y')\n",
    "r_variables = ['month', 'y']\n",
    "\n",
    "target = df_PCA['y']\n",
    "#drop variables\n",
    "explainatory = df_PCA.drop(r_variables, axis=1)\n",
    "\n",
    "#scale explanatory variables (mean = 0, variance = 1)\n",
    "scaler = StandardScaler().fit(explainatory)\n",
    "\n",
    "# Apply transform to variables.\n",
    "train = scaler.transform(explainatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(explainatory)\n",
    "pca_t = pca.transform(explainatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plot component 1 vs component 2\n",
    "f = plt.figure()\n",
    "plt.scatter(pca_t[:, 0], pca_t[:, 1], c=target, marker='.', cmap='jet')\n",
    "plt.title('PC2 vs PC1')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "f.savefig(\"scatter.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe\n",
    "\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "f = plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Explained Variance')\n",
    "\n",
    "f.savefig(\"variance.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(pca_t[:,])\n",
    "pca_df.to_csv(\"pca_df.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "#### too computationally expensive to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "#Source: https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/\n",
    "\n",
    "#random.seed(2019)\n",
    "# split data into training and test set\n",
    "#train, test, train_target, test_target = train_test_split(pca_df, target, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linearly separable data\n",
    "\n",
    "#svclassifier = SVC(kernel='linear')\n",
    "#svclassifier.fit(train, train_target)\n",
    "\n",
    "#y_pred = svclassifier.predict(test)\n",
    "\n",
    "\n",
    "#print(confusion_matrix(test_target,y_pred))\n",
    "#print(classification_report(test_target,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-linearly separable data, gaussian kernel\n",
    "\n",
    "#svclassifier = SVC(kernel='rbf')\n",
    "#svclassifier.fit(train, train_target)\n",
    "\n",
    "#y_pred = svclassifier.predict(test)\n",
    "\n",
    "\n",
    "#print(confusion_matrix(test_target,y_pred))\n",
    "#print(classification_report(test_target,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-linearly separable data, sigmoid kernel\n",
    "\n",
    "#svclassifier = SVC(kernel='sigmoid')\n",
    "#svclassifier.fit(train, train_target)\n",
    "\n",
    "#y_pred = svclassifier.predict(test)\n",
    "\n",
    "\n",
    "#print(confusion_matrix(test_target,y_pred))\n",
    "#print(classification_report(test_target,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-linearly separable data, polynomial kernel\n",
    "\n",
    "#for degree in range(2,6):\n",
    "#    svclassifier = SVC(kernel='poly', degree=degree)\n",
    "#    svclassifier.fit(train, train_target)\n",
    "\n",
    "#    y_pred = svclassifier.predict(test)\n",
    "\n",
    "#    print(degree)\n",
    "#    print(confusion_matrix(test_target,y_pred))\n",
    "#    print(classification_report(test_target,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN - full dataset (minus EDA findings) - k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_df=df.drop(['x41', 'x6', 'month', 'continent', 'day','x32'], axis=1)\n",
    "\n",
    "# Fill in NA with mean - LR needs values in each cell \n",
    "#knn_df = knn_df.fillna(df.mean())\n",
    "\n",
    "# Alternative - Drop all rows with NA\n",
    "knn_df = knn_df.dropna()\n",
    "\n",
    "#Create x and y\n",
    "\n",
    "y = knn_df['y']\n",
    "x = knn_df.drop('y', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TrainTestSplit\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Predictions\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Results\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy = 79.39%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Loop k=1-25 to find the best K value - using subset of data from Random Forest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset by RF important features >.04\n",
    "knn_importance_df = knn_df[['x23', 'x20', 'x48', 'x49', 'x38', 'x12', 'x42', 'x27','x40', 'x37','x28','x7','x2', 'x46', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create x and y\n",
    "\n",
    "y = knn_importance_df['y']\n",
    "x = knn_importance_df.drop('y', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TrainTestSplit\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://www.ritchieng.com/machine-learning-k-nearest-neighbors-knn/\n",
    "# try K=1 through K=25 and record testing accuracy\n",
    "k_range = range(1, 26)\n",
    "\n",
    "# We can create Python dictionary using [] or dict()\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train, y_train)\n",
    "    y_pred = knn.predict(x_test)\n",
    "    scores.append(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize it \n",
    "# Source: https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 26), scores, color='red', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "plt.title('Accuracy By K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('KNN_Accuracy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best accuracy - k=9\n",
    "\n",
    "## kNN - k=9 on subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create x and y\n",
    "\n",
    "y = knn_importance_df['y']\n",
    "x = knn_importance_df.drop('y', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TrainTestSplit\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Predictions\n",
    "classifier = KNeighborsClassifier(n_neighbors=9)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Results\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = np.array([[18394, 714],\n",
    "                   [831, 12061]])\n",
    "\n",
    "class_names = ['y=0', 'y=1']\n",
    "fig, ax = plot_confusion_matrix(conf_mat=binary,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,\n",
    "                               class_names=class_names,\n",
    "                               figsize=(8,8))\n",
    "plt.title('KNN Confusion Matrix')\n",
    "plt.savefig('KNN_Matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy = 95.17%\n",
    "\n",
    "## kNN with top 8 features from PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV\n",
    "df = pd.read_csv(\"pca_df.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCAdf=df[['0', '1', '2', '3', '4', '5', '6', '7', '8', 'y']]\n",
    "PCAdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create x and y\n",
    "\n",
    "y = PCAdf['y']\n",
    "x = PCAdf.drop('y', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TrainTestSplit\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Predictions\n",
    "classifier = KNeighborsClassifier(n_neighbors=9)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Results\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy = 85.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_xg = knn_importance_df['y']\n",
    "x_xg = knn_importance_df.drop('y', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TrainTestSplit\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_xg, y_xg, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
