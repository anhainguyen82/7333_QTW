{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center\"> DS7333 - Case Study 6 | SPAM</div>\n",
    "### <div style=\"text-align:center\">Andy Ho, An Nguyen, Jodi Pafford</div>\n",
    "<div style=\"text-align:center\">June 17, 2019</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "+ Nolan, D. and Lang, D. T. “Data Science in R.” CRC Press, 2015 (Chapter 1)\n",
    "+ http://rdatasciencecases.org/\n",
    "+ https://rstudio-pubs-static.s3.amazonaws.com/351788_b8d5de284dd645a1b920b7bd77e0967b.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Appendix - Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions to choose from\n",
    "19. Consider the other parameters that can be used to control the recursive partitioning process. Read the documentation for them in the rpart.control() documentation. Also, carry out an Internet search for more information on how to tweak the rpart() tuning parameters. Experiment with values for these parameters. Do the trees that result make sense with your understanding of how the parameters are used? Can you improve the prediction using them?\n",
    "\n",
    "20. In the section called “Classifying New Messages” we used the test set that we had put aside to both select , the threshold for the log odds, and to evaluate the Type I and II errors incurred when we use this threshold. Ideally, we choose from another set of messages that is both independent of our training data and our test data. The method of cross-validation is designed to use the training set for training and validating the model. Implement 5-fold cross-validation to choose and assess the error rate with our training data. To do this, follow the steps:\n",
    "\n",
    "        a. Use the sample() function to permute the indices of the training set, and organize these permuted indices into 5 equal-size sets, called folds.\n",
    "\n",
    "        b. For each fold, take the corresponding subset from the training data to use as a 'test' set. Use the remaining messages in the training data as the training set. Apply the functions developed in the section called “Implementing the Naïve Bayes Classifier” to estimate the probabilities that a word occurs in a message given it is spam or ham, and use these probabilities to compute the log likelihood ratio for the messages in the training set.\n",
    "\n",
    "        c. Pool all of the LLR values from the messages in all of the folds, i.e., from all of the training data, and use these values and the typeIErrorRate() function to select a threshold that achieves a 1% Type I error.\n",
    "\n",
    "        d. Apply this threshold to our original/real test set and find its Type I and Type II errors.\n",
    "\n",
    "21. Use the sample() function to permute the indices of the training set, and organize these permuted indices into 5 equal-size sets, called folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so you can change the dir structure, but this code does depend on data being in certain directories:\n",
    "\n",
    "\n",
    "`This Notebook`\n",
    "\n",
    "`    < spam path folder > `\n",
    "\n",
    "        messages\n",
    "    \n",
    "            easy_ham\n",
    "        \n",
    "            spam\n",
    "        \n",
    "            etc....\n",
    "        `\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamPath = \"./SpamAssassinMessages/\"\n",
    "dirNames = list.files(path = paste(spamPath, \"messages\",  sep = .Platform$file.sep))\n",
    "fullDirNames = paste(spamPath, \"messages\", dirNames, sep = .Platform$file.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitMessage = function(msg) {\n",
    "  splitPoint = match(\"\", msg)\n",
    "  header = msg[1:(splitPoint-1)]\n",
    "  body = msg[ -(1:splitPoint) ]\n",
    "  return(list(header = header, body = body))\n",
    "}\n",
    "\n",
    "getBoundary = function(header) {\n",
    "  boundaryIdx = grep(\"boundary=\", header)\n",
    "  boundary = gsub('\"', \"\", header[boundaryIdx])\n",
    "  gsub(\".*boundary= *([^;]*);?.*\", \"\\\\1\", boundary)\n",
    "}\n",
    "\n",
    "dropAttach = function(body, boundary){\n",
    "  \n",
    "  bString = paste(\"--\", boundary, sep = \"\")\n",
    "  bStringLocs = which(bString == body)\n",
    "  \n",
    "  if (length(bStringLocs) <= 1) return(body)\n",
    "  \n",
    "  eString = paste(\"--\", boundary, \"--\", sep = \"\")\n",
    "  eStringLoc = which(eString == body)\n",
    "  if (length(eStringLoc) == 0) \n",
    "    return(body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)])\n",
    "  \n",
    "  n = length(body)\n",
    "  if (eStringLoc < n) \n",
    "     return( body[ c( (bStringLocs[1] + 1) : (bStringLocs[2] - 1), \n",
    "                    ( (eStringLoc + 1) : n )) ] )\n",
    "  \n",
    "  return( body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1) ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'tm' was built under R version 3.4.4\"Loading required package: NLP\n"
     ]
    }
   ],
   "source": [
    "library(tm)\n",
    "\n",
    "stopWords = stopwords()\n",
    "cleanSW = tolower(gsub(\"[[:punct:]0-9[:blank:]]+\", \" \", stopWords))\n",
    "SWords = unlist(strsplit(cleanSW, \"[[:blank:]]+\"))\n",
    "SWords = SWords[ nchar(SWords) > 1 ]\n",
    "stopWords = unique(SWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanText =\n",
    "function(msg)   {\n",
    "  tolower(gsub(\"[[:punct:]0-9[:space:][:blank:]]+\", \" \", msg))\n",
    "}\n",
    "\n",
    "findMsgWords = \n",
    "function(msg, stopWords) {\n",
    " if(is.null(msg))\n",
    "  return(character())\n",
    "\n",
    " words = unique(unlist(strsplit(cleanText(msg), \"[[:blank:]\\t]+\")))\n",
    " \n",
    " # drop empty and 1 letter words\n",
    " words = words[ nchar(words) > 1]\n",
    " words = words[ !( words %in% stopWords) ]\n",
    " invisible(words)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processAllWords = function(dirName, stopWords)\n",
    "{\n",
    "       # read all files in the directory\n",
    "  fileNames = list.files(dirName, full.names = TRUE)\n",
    "       # drop files that are not email, i.e., cmds\n",
    "  notEmail = grep(\"cmds$\", fileNames)\n",
    "  if ( length(notEmail) > 0) fileNames = fileNames[ - notEmail ]\n",
    "\n",
    "  messages = lapply(fileNames, readLines, encoding = \"latin1\")\n",
    "  \n",
    "       # split header and body\n",
    "  emailSplit = lapply(messages, splitMessage)\n",
    "       # put body and header in own lists\n",
    "  bodyList = lapply(emailSplit, function(msg) msg$body)\n",
    "  headerList = lapply(emailSplit, function(msg) msg$header)\n",
    "  rm(emailSplit)\n",
    "  \n",
    "       # determine which messages have attachments\n",
    "  hasAttach = sapply(headerList, function(header) {\n",
    "    CTloc = grep(\"Content-Type\", header)\n",
    "    if (length(CTloc) == 0) return(0)\n",
    "    multi = grep(\"multi\", tolower(header[CTloc])) \n",
    "    if (length(multi) == 0) return(0)\n",
    "    multi\n",
    "  })\n",
    "  \n",
    "  hasAttach = which(hasAttach > 0)\n",
    "  \n",
    "       # find boundary strings for messages with attachments\n",
    "  boundaries = sapply(headerList[hasAttach], getBoundary)\n",
    "  \n",
    "       # drop attachments from message body\n",
    "  bodyList[hasAttach] = mapply(dropAttach, bodyList[hasAttach], \n",
    "                               boundaries, SIMPLIFY = FALSE)\n",
    "  \n",
    "       # extract words from body\n",
    "  msgWordsList = lapply(bodyList, findMsgWords, stopWords)\n",
    "  \n",
    "  invisible(msgWordsList)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './SpamAssassinMessages//messages/spam/00136.faa39d8e816c70f23b4bb8758d8a74f0'\"Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './SpamAssassinMessages//messages/spam/0143.260a940290dcb61f9327b224a368d4af'\""
     ]
    }
   ],
   "source": [
    "msgWordsList = lapply(fullDirNames, processAllWords, \n",
    "                      stopWords = stopWords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>5051</li>\n",
       "\t<li>1400</li>\n",
       "\t<li>500</li>\n",
       "\t<li>1000</li>\n",
       "\t<li>1397</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 5051\n",
       "\\item 1400\n",
       "\\item 500\n",
       "\\item 1000\n",
       "\\item 1397\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 5051\n",
       "2. 1400\n",
       "3. 500\n",
       "4. 1000\n",
       "5. 1397\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 5051 1400  500 1000 1397"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numMsgs = sapply(msgWordsList, length)\n",
    "numMsgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "isSpam = rep(c(FALSE, FALSE, FALSE, TRUE, TRUE), numMsgs)\n",
    "\n",
    "msgWordsList = unlist(msgWordsList, recursive = FALSE)\n",
    "\n",
    "numEmail = length(isSpam)\n",
    "numSpam = sum(isSpam)\n",
    "numHam = numEmail - numSpam\n",
    "\n",
    "set.seed(418910)\n",
    "\n",
    "#number of folds to produce\n",
    "fold = 5\n",
    "\n",
    "#initialize lists of indexes\n",
    "foldSpamIdx = list()\n",
    "foldHamIdx = list()\n",
    "\n",
    "#create a list of x folds holding a list of indexes\n",
    "#sample() will not sample indexes that have already been taken for previous folds\n",
    "for (x in 1:fold){\n",
    "    foldSpamIdx[x] = list(sample((1:numSpam)[!((1:numSpam) %in% unlist(foldSpamIdx))], size = floor(numSpam/fold)))\n",
    "    foldHamIdx[x] = list(sample((1:numHam)[!((1:numHam) %in% unlist(foldHamIdx))], size = floor(numHam/fold)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeFreqs =\n",
    "function(wordsList, spam, bow = unique(unlist(wordsList)))\n",
    "{\n",
    "   # create a matrix for spam, ham, and log odds\n",
    "  wordTable = matrix(0.5, nrow = 4, ncol = length(bow), \n",
    "                     dimnames = list(c(\"spam\", \"ham\", \n",
    "                                        \"presentLogOdds\", \n",
    "                                        \"absentLogOdds\"),  bow))\n",
    "\n",
    "   # For each spam message, add 1 to counts for words in message\n",
    "  counts.spam = table(unlist(lapply(wordsList[spam], unique)))\n",
    "  wordTable[\"spam\", names(counts.spam)] = counts.spam + .5\n",
    "\n",
    "   # Similarly for ham messages\n",
    "  counts.ham = table(unlist(lapply(wordsList[!spam], unique)))  \n",
    "  wordTable[\"ham\", names(counts.ham)] = counts.ham + .5  \n",
    "\n",
    "\n",
    "   # Find the total number of spam and ham\n",
    "  numSpam = sum(spam)\n",
    "  numHam = length(spam) - numSpam\n",
    "\n",
    "   # Prob(word|spam) and Prob(word | ham)\n",
    "  wordTable[\"spam\", ] = wordTable[\"spam\", ]/(numSpam + .5)\n",
    "  wordTable[\"ham\", ] = wordTable[\"ham\", ]/(numHam + .5)\n",
    "  \n",
    "   # log odds\n",
    "  wordTable[\"presentLogOdds\", ] = \n",
    "     log(wordTable[\"spam\",]) - log(wordTable[\"ham\", ])\n",
    "  wordTable[\"absentLogOdds\", ] = \n",
    "     log((1 - wordTable[\"spam\", ])) - log((1 -wordTable[\"ham\", ]))\n",
    "\n",
    "  invisible(wordTable)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of word table, 1 for each fold as the test set\n",
    "trainTable = list()\n",
    "testMsgWords = list()\n",
    "testIsSpam = list()\n",
    "trainIsSpam = list()\n",
    "\n",
    "for (x in 1:fold){\n",
    "    testSpamIdx = foldSpamIdx[[x]]\n",
    "    testHamIdx = foldHamIdx[[x]]\n",
    "\n",
    "    testMsgWords[[x]] = c((msgWordsList[isSpam])[testSpamIdx],\n",
    "                     (msgWordsList[!isSpam])[testHamIdx] )\n",
    "    trainMsgWords = c((msgWordsList[isSpam])[ - testSpamIdx], \n",
    "                      (msgWordsList[!isSpam])[ - testHamIdx])\n",
    "\n",
    "    testIsSpam[[x]] = rep(c(TRUE, FALSE), \n",
    "                     c(length(testSpamIdx), length(testHamIdx)))\n",
    "    trainIsSpam[[x]] = rep(c(TRUE, FALSE), \n",
    "                     c(numSpam - length(testSpamIdx), \n",
    "                       numHam - length(testHamIdx)))\n",
    "\n",
    "    trainTable[[x]] = list(computeFreqs(trainMsgWords, trainIsSpam[[x]]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeMsgLLR = function(words, freqTable) \n",
    "{\n",
    "       # Discards words not in training data.\n",
    "  words = words[!is.na(match(words, colnames(freqTable)))]\n",
    "\n",
    "       # Find which words are present\n",
    "  present = colnames(freqTable) %in% words\n",
    "\n",
    "  sum(freqTable[\"presentLogOdds\", present]) +\n",
    "    sum(freqTable[\"absentLogOdds\", !present])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Fold 1:\"\n",
      "$`FALSE`\n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      "-844.88 -126.92  -98.05 -116.89  -77.28  746.25 \n",
      "\n",
      "$`TRUE`\n",
      "    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n",
      "  -56.55    13.64    58.65   179.92   146.84 23524.91 \n",
      "\n",
      "[1] \"Fold 2:\"\n",
      "$`FALSE`\n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      "-866.19 -135.78 -108.61 -125.88  -86.79   95.79 \n",
      "\n",
      "$`TRUE`\n",
      "    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n",
      " -75.653    2.039   48.668  101.734  128.926 1446.490 \n",
      "\n",
      "[1] \"Fold 3:\"\n",
      "$`FALSE`\n",
      "    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n",
      "-1099.20  -134.40  -105.34  -123.34   -83.98    14.48 \n",
      "\n",
      "$`TRUE`\n",
      "     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n",
      "  -73.267     2.469    56.071   146.890   122.165 23516.206 \n",
      "\n",
      "[1] \"Fold 4:\"\n",
      "$`FALSE`\n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      "-696.79 -132.56 -105.14 -122.86  -85.08  107.62 \n",
      "\n",
      "$`TRUE`\n",
      "    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n",
      " -65.873    7.451   55.138  119.337  134.529 3450.687 \n",
      "\n",
      "[1] \"Fold 5:\"\n",
      "$`FALSE`\n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      "-956.90 -138.04 -110.85 -128.20  -89.15   73.53 \n",
      "\n",
      "$`TRUE`\n",
      "    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n",
      " -60.353    1.221   42.270   90.864  115.221 1445.668 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "testLLR = list()\n",
    "for (x in 1:fold){\n",
    "    testLLR[[x]] = sapply(testMsgWords[[x]], computeMsgLLR, trainTable[[x]][[1]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeIErrorRates = \n",
    "function(llrVals, isSpam) \n",
    "{\n",
    "  o = order(llrVals)\n",
    "  llrVals =  llrVals[o]\n",
    "  isSpam = isSpam[o]\n",
    "\n",
    "  idx = which(!isSpam)\n",
    "  N = length(idx)\n",
    "  list(error = (N:1)/N, values = llrVals[idx])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "-6"
      ],
      "text/latex": [
       "-6"
      ],
      "text/markdown": [
       "-6"
      ],
      "text/plain": [
       "[1] -6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xI = typeIErrorRates(testLLR[[1]], testIsSpam[[1]])\n",
    "tau01 = round(min(xI$values[xI$error <= 0.01]))\n",
    "tau01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
