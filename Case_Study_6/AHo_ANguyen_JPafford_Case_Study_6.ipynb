{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center\"> DS7333 - Case Study 6 | SPAM</div>\n",
    "### <div style=\"text-align:center\">Andy Ho, An Nguyen, Jodi Pafford</div>\n",
    "<div style=\"text-align:center\">June 17, 2019</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "This case study will answer Question 20 in Chapter 3 of \"Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving\" (Nolan and Lang). \n",
    "\n",
    ">Question 20 is: \"In the section called “Classifying New Messages” we used the test set that we had put aside to both select , the threshold for the log odds, and to evaluate the Type I and II errors incurred when we use this threshold. Ideally, we choose from another set of messages that is both independent of our training data and our test data. The method of cross-validation is designed to use the training set for training and validating the model. Implement 5-fold cross-validation to choose and assess the error rate with our training data. To do this, follow the steps: \n",
    "    <br> \n",
    "    <br> A.) Use the sample() function to permute the indices of the training set, and organize these permuted indices into 5 equal-size sets, called folds. \n",
    "    <br> \n",
    "    <br> B.) For each fold, take the corresponding subset from the training data to use as a 'test' set. Use the remaining messages in the training data as the training set. Apply the functions developed in the section called “Implementing the Naïve Bayes Classifier” to estimate the probabilities that a word occurs in a message given it is spam or ham, and use these probabilities to compute the log likelihood ratio for the messages in the training set.\n",
    "    <br> \n",
    "    <br> C.) Pool all of the LLR values from the messages in all of the folds, i.e., from all of the training data, and use these values and the typeIErrorRate() function to select a threshold that achieves a 1% Type I error.\n",
    "    <br> \n",
    "    <br> D.) Apply this threshold to our original/real test set and find its Type I and Type II errors.\"\n",
    "\n",
    "The data was extracted, cleaned, reformatted and prepared for analysis.  The preparation includes parsing out words from the body of emails and removing stop words. Once the data was prepared, we set aside one third of the data to be used as a validation set.  The remaining two third of the data used as the training set.  The training set was further divided into 5 folds for cross-validation of the Naive-Bayes classification method in determining spam and non-spam emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Background\n",
    "\n",
    "Spam messages have been sent to email accounts almost since the inception of emails. People have gotten good at recognizing Spam messages, however AI should be able to create filters to also recognize Spam. Doing so would save companies from the risk of employees who aren't as 'savy' clicking links inside Spam emails, potentially saving the company time and money. \n",
    "\n",
    "Before an AI can learn to classify messages, programmers (people) must do some basic analysis to find the right parameters to train the AI. Does the computer simply need to look at the sending email address, or does it also need to include the entire message? How many Type I and Type II errors are found in doing any kind of filtering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Methods\n",
    "\n",
    "Following the code from Chapter 3 Sections 3.1 - 3.5, we cleaned the email files. The first step was removing the stop words and punctuation. Once this was complete, we were able to split the messages into Ham and Spam and combine into one file. The final step in preparation was to create a test and train set in order to complete the steps of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A)\n",
    "To create the 5 folds we first created two 5-element lists, one for spam emails and one for ham (non-spam emails).  We used the sample() function to randomly sample ```r floor(trainNumSpam/fold) ``` [1] for the spam emails and ```r floor(trainNumSpam/fold) ``` [2] in the ham emails training set.  With each new fold the index that have already been allocated is removed from the elements from which sample() is choosing from.\n",
    "\n",
    "[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "319"
      ],
      "text/latex": [
       "319"
      ],
      "text/markdown": [
       "319"
      ],
      "text/plain": [
       "[1] 319"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run appendix code first\n",
    "floor(trainNumSpam/fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "319"
      ],
      "text/latex": [
       "319"
      ],
      "text/markdown": [
       "319"
      ],
      "text/plain": [
       "[1] 319"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run appendix code first\n",
    "floor(trainNumSpam/fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B)\n",
    "Once the indexes have been created. For each fold the indexes are then used to create a list of words from the designated test emails, labeling them as ham or spam. The remaining words are used to create a frequency table from which the likelihood-ratio(LLR) are calculated using the words in test set. This process is repeated over the 5 folds resulting in a list of 5 lists of LLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C)\n",
    "We then determine the error rate for each word. For each fold all the LLR with error greater than 1% is deteremined. The tau is the minimum LLR from this set. Once all five taus are found we averaged.\n",
    "\n",
    "Setting the Type I error rate at 1%, we found a tau of 387.465811932302."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D)\n",
    "The Type I and II error is calculated by applying the validation test set to each of the five training set created above. The error rates are determined and then averaged together.\n",
    "\n",
    "When applied to the validation data set we see that the Type I error is indeed 1% and the Type II error is 8%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "+ Nolan, D. and Lang, D. T. “Data Science in R.” CRC Press, 2015 (Chapter 3)\n",
    "+ http://rdatasciencecases.org/\n",
    "+ https://rstudio-pubs-static.s3.amazonaws.com/351788_b8d5de284dd645a1b920b7bd77e0967b.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Appendix - Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so you can change the dir structure, but this code does depend on data being in certain directories:\n",
    "\n",
    "\n",
    "`This Notebook`\n",
    "\n",
    "`    < spam path folder > `\n",
    "\n",
    "        messages\n",
    "    \n",
    "            easy_ham\n",
    "        \n",
    "            spam\n",
    "        \n",
    "            etc....\n",
    "        `\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files\n",
    "spamPath = \"./SpamAssassinMessages/\"\n",
    "#List of messages\n",
    "dirNames = list.files(path = paste(spamPath, \"messages\",  sep = .Platform$file.sep))\n",
    "#Findn file names and place in directories\n",
    "fullDirNames = paste(spamPath, \"messages\", dirNames, sep = .Platform$file.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split message into 2 character vectors (header and body)\n",
    "splitMessage = function(msg) {\n",
    "  splitPoint = match(\"\", msg)\n",
    "  header = msg[1:(splitPoint-1)]\n",
    "  body = msg[ -(1:splitPoint) ]\n",
    "  return(list(header = header, body = body))\n",
    "}\n",
    "\n",
    "#find the boundary string\n",
    "getBoundary = function(header) {\n",
    "  boundaryIdx = grep(\"boundary=\", header)\n",
    "  boundary = gsub('\"', \"\", header[boundaryIdx])\n",
    "  gsub(\".*boundary= *([^;]*);?.*\", \"\\\\1\", boundary)\n",
    "}\n",
    "\n",
    "#drop attachment using boundary info above\n",
    "dropAttach = function(body, boundary){\n",
    "  \n",
    "  bString = paste(\"--\", boundary, sep = \"\")\n",
    "  bStringLocs = which(bString == body)\n",
    "  \n",
    "  if (length(bStringLocs) <= 1) return(body)\n",
    "  \n",
    "  eString = paste(\"--\", boundary, \"--\", sep = \"\")\n",
    "  eStringLoc = which(eString == body)\n",
    "  if (length(eStringLoc) == 0) \n",
    "    return(body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)])\n",
    "  \n",
    "  n = length(body)\n",
    "  if (eStringLoc < n) \n",
    "     return( body[ c( (bStringLocs[1] + 1) : (bStringLocs[2] - 1), \n",
    "                    ( (eStringLoc + 1) : n )) ] )\n",
    "  \n",
    "  return( body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1) ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'tm' was built under R version 3.4.4\"Loading required package: NLP\n"
     ]
    }
   ],
   "source": [
    "library(tm)\n",
    "\n",
    "stopWords = stopwords()\n",
    "\n",
    "#clean to stop words to remove case and punctuation\n",
    "cleanSW = tolower(gsub(\"[[:punct:]0-9[:blank:]]+\", \" \", stopWords))\n",
    "\n",
    "#divide stop words into strings into words by splitting the string on blanks\n",
    "SWords = unlist(strsplit(cleanSW, \"[[:blank:]]+\"))\n",
    "\n",
    "#Drop one-letter stop words\n",
    "SWords = SWords[ nchar(SWords) > 1 ]\n",
    "\n",
    "stopWords = unique(SWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final clean text code (like above but in a function for the message words)\n",
    "cleanText =\n",
    "function(msg)   {\n",
    "  tolower(gsub(\"[[:punct:]0-9[:space:][:blank:]]+\", \" \", msg))\n",
    "}\n",
    "\n",
    "findMsgWords = \n",
    "function(msg, stopWords) {\n",
    " if(is.null(msg))\n",
    "  return(character())\n",
    "\n",
    " words = unique(unlist(strsplit(cleanText(msg), \"[[:blank:]\\t]+\")))\n",
    " \n",
    " # drop empty and 1 letter words\n",
    " words = words[ nchar(words) > 1]\n",
    " words = words[ !( words %in% stopWords) ]\n",
    " invisible(words)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the entire group of messages in one larger function that contains the functions of the others\n",
    "processAllWords = function(dirName, stopWords)\n",
    "{\n",
    "       # read all files in the directory\n",
    "  fileNames = list.files(dirName, full.names = TRUE)\n",
    "       # drop files that are not email, i.e., cmds\n",
    "  notEmail = grep(\"cmds$\", fileNames)\n",
    "  if ( length(notEmail) > 0) fileNames = fileNames[ - notEmail ]\n",
    "\n",
    "  messages = lapply(fileNames, readLines, encoding = \"latin1\")\n",
    "  \n",
    "       # split header and body\n",
    "  emailSplit = lapply(messages, splitMessage)\n",
    "       # put body and header in own lists\n",
    "  bodyList = lapply(emailSplit, function(msg) msg$body)\n",
    "  headerList = lapply(emailSplit, function(msg) msg$header)\n",
    "  rm(emailSplit)\n",
    "  \n",
    "       # determine which messages have attachments\n",
    "  hasAttach = sapply(headerList, function(header) {\n",
    "    CTloc = grep(\"Content-Type\", header)\n",
    "    if (length(CTloc) == 0) return(0)\n",
    "    multi = grep(\"multi\", tolower(header[CTloc])) \n",
    "    if (length(multi) == 0) return(0)\n",
    "    multi\n",
    "  })\n",
    "  \n",
    "  hasAttach = which(hasAttach > 0)\n",
    "  \n",
    "       # find boundary strings for messages with attachments\n",
    "  boundaries = sapply(headerList[hasAttach], getBoundary)\n",
    "  \n",
    "       # drop attachments from message body\n",
    "  bodyList[hasAttach] = mapply(dropAttach, bodyList[hasAttach], \n",
    "                               boundaries, SIMPLIFY = FALSE)\n",
    "  \n",
    "       # extract words from body\n",
    "  msgWordsList = lapply(bodyList, findMsgWords, stopWords)\n",
    "  \n",
    "  invisible(msgWordsList)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './SpamAssassinMessages//messages/spam/00136.faa39d8e816c70f23b4bb8758d8a74f0'\"Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './SpamAssassinMessages//messages/spam/0143.260a940290dcb61f9327b224a368d4af'\""
     ]
    }
   ],
   "source": [
    "#apply the 'processAllWords to the entire directory\n",
    "msgWordsList = lapply(fullDirNames, processAllWords, \n",
    "                      stopWords = stopWords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>5051</li>\n",
       "\t<li>1400</li>\n",
       "\t<li>500</li>\n",
       "\t<li>1000</li>\n",
       "\t<li>1397</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 5051\n",
       "\\item 1400\n",
       "\\item 500\n",
       "\\item 1000\n",
       "\\item 1397\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 5051\n",
       "2. 1400\n",
       "3. 500\n",
       "4. 1000\n",
       "5. 1397\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 5051 1400  500 1000 1397"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#vector of the number of elements in each list\n",
    "numMsgs = sapply(msgWordsList, length)\n",
    "numMsgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notating which is spam and which is ham\n",
    "isSpam = rep(c(FALSE, FALSE, FALSE, TRUE, TRUE), numMsgs)\n",
    "\n",
    "#one list of all words (all 5 files combined)\n",
    "msgWordsList = unlist(msgWordsList, recursive = FALSE)\n",
    "\n",
    "#number of emails\n",
    "numEmail = length(isSpam)\n",
    "#number of spam emails\n",
    "numSpam = sum(isSpam)\n",
    "#number of ham emails\n",
    "numHam = numEmail - numSpam\n",
    "\n",
    "set.seed(418910)\n",
    "\n",
    "#determine indices\n",
    "validationSpamIdx = sample(numSpam, size = floor(numSpam/3))\n",
    "validationHamIdx = sample(numHam, size = floor(numHam/3))\n",
    "\n",
    "#select word vectors\n",
    "validationMsgWords = c((msgWordsList[isSpam])[validationSpamIdx],\n",
    "                 (msgWordsList[!isSpam])[validationHamIdx] )\n",
    "trainingMsgWords = c((msgWordsList[isSpam])[ - validationSpamIdx], \n",
    "                  (msgWordsList[!isSpam])[ - validationHamIdx])\n",
    "\n",
    "#create test (validation) and train\n",
    "validationIsSpam = rep(c(TRUE, FALSE), \n",
    "                 c(length(validationSpamIdx), length(validationHamIdx)))\n",
    "trainingIsSpam = rep(c(TRUE, FALSE), \n",
    "                 c(numSpam - length(validationSpamIdx), \n",
    "                   numHam - length(validationHamIdx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#number of folds to produce\n",
    "fold = 5\n",
    "\n",
    "trainNumEmail = length(trainingIsSpam)\n",
    "trainNumSpam = sum(trainingIsSpam)\n",
    "trainNumHam = trainNumEmail - trainNumSpam\n",
    "\n",
    "#initialize lists of indexes\n",
    "foldSpamIdx = list()\n",
    "foldHamIdx = list()\n",
    "\n",
    "#create a list of x folds holding a list of indexes\n",
    "#sample() will not sample indexes that have already been taken for previous folds\n",
    "for (x in 1:fold){\n",
    "    foldSpamIdx[x] = list(sample((1:trainNumSpam)[!((1:trainNumSpam) %in% unlist(foldSpamIdx))], size = floor(trainNumSpam/fold)))\n",
    "    foldHamIdx[x] = list(sample((1:trainNumHam)[!((1:trainNumHam) %in% unlist(foldHamIdx))], size = floor(trainNumHam/fold)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to form a matrix of log likelihood rations and porportions\n",
    "computeFreqs =\n",
    "function(wordsList, spam, bow = unique(unlist(wordsList)))\n",
    "{\n",
    "   # create a matrix for spam, ham, and log odds\n",
    "  wordTable = matrix(0.5, nrow = 4, ncol = length(bow), \n",
    "                     dimnames = list(c(\"spam\", \"ham\", \n",
    "                                        \"presentLogOdds\", \n",
    "                                        \"absentLogOdds\"),  bow))\n",
    "\n",
    "   # For each spam message, add 1 to counts for words in message\n",
    "  counts.spam = table(unlist(lapply(wordsList[spam], unique)))\n",
    "  wordTable[\"spam\", names(counts.spam)] = counts.spam + .5\n",
    "\n",
    "   # Similarly for ham messages\n",
    "  counts.ham = table(unlist(lapply(wordsList[!spam], unique)))  \n",
    "  wordTable[\"ham\", names(counts.ham)] = counts.ham + .5  \n",
    "\n",
    "\n",
    "   # Find the total number of spam and ham\n",
    "  numSpam = sum(spam)\n",
    "  numHam = length(spam) - numSpam\n",
    "\n",
    "   # Prob(word|spam) and Prob(word | ham)\n",
    "  wordTable[\"spam\", ] = wordTable[\"spam\", ]/(numSpam + .5)\n",
    "  wordTable[\"ham\", ] = wordTable[\"ham\", ]/(numHam + .5)\n",
    "  \n",
    "   # log odds\n",
    "  wordTable[\"presentLogOdds\", ] = \n",
    "     log(wordTable[\"spam\",]) - log(wordTable[\"ham\", ])\n",
    "  wordTable[\"absentLogOdds\", ] = \n",
    "     log((1 - wordTable[\"spam\", ])) - log((1 - wordTable[\"ham\", ]))\n",
    "\n",
    "  invisible(wordTable)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of word table, 1 for each fold as the test set\n",
    "trainTable = list()\n",
    "testMsgWords = list()\n",
    "testIsSpam = list()\n",
    "trainIsSpam = list()\n",
    "\n",
    "for (x in 1:fold){\n",
    "    testSpamIdx = foldSpamIdx[[x]]\n",
    "    testHamIdx = foldHamIdx[[x]]\n",
    "\n",
    "    testMsgWords[[x]] = c((trainingMsgWords[trainingIsSpam])[testSpamIdx],\n",
    "                     (trainingMsgWords[!trainingIsSpam])[testHamIdx] )\n",
    "    trainMsgWords = c((trainingMsgWords[trainingIsSpam])[ - testSpamIdx], \n",
    "                      (trainingMsgWords[!trainingIsSpam])[ - testHamIdx])\n",
    "\n",
    "    testIsSpam[[x]] = rep(c(TRUE, FALSE), \n",
    "                     c(length(testSpamIdx), length(testHamIdx)))\n",
    "    trainIsSpam[[x]] = rep(c(TRUE, FALSE), \n",
    "                     c(numSpam - length(testSpamIdx), \n",
    "                       numHam - length(testHamIdx)))\n",
    "\n",
    "    trainTable[[x]] = list(computeFreqs(trainMsgWords, trainIsSpam[[x]]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the log likelihood ration (LLR)\n",
    "computeMsgLLR = function(words, freqTable) \n",
    "{\n",
    "       # Discards words not in training data.\n",
    "  words = words[!is.na(match(words, colnames(freqTable)))]\n",
    "\n",
    "       # Find which words are present\n",
    "  present = colnames(freqTable) %in% words\n",
    "\n",
    "  sum(freqTable[\"presentLogOdds\", present]) +\n",
    "    sum(freqTable[\"absentLogOdds\", !present])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Log Likelihood Ratio (LLR) for Test Messages\n",
    "testLLR = list()\n",
    "for (x in 1:fold){\n",
    "    testLLR[[x]] = sapply(testMsgWords[[x]], computeMsgLLR, trainTable[[x]][[1]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the Type I Error Rate (how many ham are classified as Spam)\n",
    "typeIErrorRates = \n",
    "function(llrVals, isSpam) \n",
    "{\n",
    "  o = order(llrVals)\n",
    "  llrVals =  llrVals[o]\n",
    "  isSpam = isSpam[o]\n",
    "\n",
    "  idx = which(!isSpam)\n",
    "  N = length(idx)\n",
    "  list(error = (N:1)/N, values = llrVals[idx])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "387.465811932302"
      ],
      "text/latex": [
       "387.465811932302"
      ],
      "text/markdown": [
       "387.465811932302"
      ],
      "text/plain": [
       "[1] 387.4658"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xI = list()\n",
    "tau01 = list()\n",
    "\n",
    "for (x in 1:fold){\n",
    "    xI[[x]] = typeIErrorRates(testLLR[[x]], testIsSpam[[x]])\n",
    "    tau01[[x]] = min(xI[[x]]$values[xI[[x]]$error <= 0.01])\n",
    "}\n",
    "\n",
    "mean_tau01 = mean(unlist(tau01))\n",
    "mean_tau01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationLLR = list()\n",
    "for (x in 1:fold){\n",
    "    validationLLR[[x]] = sapply(validationMsgWords, computeMsgLLR, trainTable[[x]][[1]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type I and Type II error rate\n",
    "typeIErrorRate = \n",
    "function(tau, llrVals, spam)\n",
    "{\n",
    "  classify = llrVals > tau\n",
    "  sum(classify & !spam)/sum(!spam)\n",
    "}\n",
    "\n",
    "typeIIErrorRate = \n",
    "function(tau, llrVals, spam)\n",
    "{\n",
    "  classify = llrVals > tau\n",
    "  sum(classify & spam)/sum(spam)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Type I error rate with tau = 387.47 is 1%.\"\n",
      "[1] \"Type II error rate with tau = 387.47 is 8%.\"\n"
     ]
    }
   ],
   "source": [
    "tIerror = list()\n",
    "tIIerror = list()\n",
    "\n",
    "for (x in 1:fold){\n",
    "    tIerror[[x]] = typeIErrorRate(mean_tau01, validationLLR[[x]], validationIsSpam) #1 if all is classify as spam\n",
    "    tIIerror[[x]] = typeIIErrorRate(mean_tau01, validationLLR[[x]], validationIsSpam) #1 if all is classify as ham\n",
    "}\n",
    "\n",
    "print(paste(\"Type I error rate with tau = \", round(mean_tau01, 2), \" is \", round(mean(unlist(tIerror))*100), \"%.\", sep=\"\"))\n",
    "print(paste(\"Type II error rate with tau = \", round(mean_tau01, 2), \" is \", round(mean(unlist(tIIerror))*100), \"%.\", sep=\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
